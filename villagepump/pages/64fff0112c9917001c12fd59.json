{
  "id": "64fff0112c9917001c12fd59",
  "title": "過学習からの汎化",
  "created": 1694494740,
  "updated": 1694494756,
  "lines": [
    {
      "id": "64fff0112c9917001c12fd59",
      "text": "過学習からの汎化",
      "userId": "6436266c7ac3c0001bed60e6",
      "created": 1694494740,
      "updated": 1694494740
    },
    {
      "id": "64fff015ed60e60000dd1f76",
      "text": ">[https://twitter.com/umiyuki_ai/status/1632563339011461121 @umiyuki_ai]: １年前にOpenAIが発表した論文の話。シンプルなニューラルネットで実験してて、最適な学習量を超えて、過学習に突入してもまだずーっと学習させてたら、ある時期から急速に汎化が起きるグロッキングという現象が発見されたという話。過学習ってのは問 https://t.co/AxPJRZttlE…",
      "userId": "6436266c7ac3c0001bed60e6",
      "created": 1694494740,
      "updated": 1694494752
    },
    {
      "id": "64fff015ed60e60000dd1f77",
      "text": "全文は以下の通り",
      "userId": "6436266c7ac3c0001bed60e6",
      "created": 1694494741,
      "updated": 1694494753
    },
    {
      "id": "64fff015ed60e60000dd1f78",
      "text": ">１年前にOpenAIが発表した論文の話。シンプルなニューラルネットで実験してて、最適な学習量を超えて、過学習に突入してもまだずーっと学習させてたら、ある時期から急速に汎化が起きるグロッキングという現象が発見されたという話。過学習ってのは問題と答えを暗記してるだけの状態だけど、汎化ってのは問題の意味を理解してるからデータセットに無いパターンの問題でも応用で答えられる状態。つまり延々と詰め込み学習を続けてたらある瞬間に急に「全部理解したわ」とか言い出した感じ　→RT　https://arxiv.org/abs/2201.02177",
      "userId": "6436266c7ac3c0001bed60e6",
      "created": 1694494741,
      "updated": 1694494754
    },
    {
      "id": "64fff015ed60e60000dd1f79",
      "text": "",
      "userId": "6436266c7ac3c0001bed60e6",
      "created": 1694494741,
      "updated": 1694494741
    }
  ]
}